# yolov5

yolov5ï¼ˆyou only look onceï¼Œversion 5ï¼‰æ˜¯åŸºäºpythonç¯å¢ƒï¼Œåœ¨pytorchæœºå™¨å­¦ä¹ æ¡†æ¶ä¸Šï¼Œä¸€ä¸ªå¼€æºçš„**ç›®æ ‡æ£€æµ‹**æ¨¡å‹ç³»åˆ—ã€‚

[yolo ç»“åˆ deepsort å®ç°ç›®æ ‡è·Ÿè¸ª](https://blog.csdn.net/Albert_yeager/article/details/129321339)

[pytorch gpu å®‰è£…](https://zhuanlan.zhihu.com/p/612181449)

# [0 åˆè¯†](https://zhuanlan.zhihu.com/p/558477653)

**yolov5 tagv5.0ç‰ˆæœ¬ä»£ç **

```bash
git clone https://github.com/ultralytics/yolov5.git
```



## 0.1 é¡¹ç›®ç»“æ„

![](./legend/yolov5é¡¹ç›®ç»“æ„.png)

â”œâ”€â”€ dataï¼šä¸»è¦æ˜¯å­˜æ”¾ä¸€äº›è¶…å‚æ•°çš„é…ç½®æ–‡ä»¶ï¼ˆè¿™äº›æ–‡ä»¶ï¼ˆyamlæ–‡ä»¶ï¼‰æ˜¯ç”¨æ¥é…ç½®è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿˜æœ‰éªŒè¯é›†çš„è·¯å¾„çš„ï¼Œå…¶ä¸­è¿˜åŒ…æ‹¬ç›®æ ‡æ£€æµ‹çš„ç§ç±»æ•°å’Œç§ç±»çš„åç§°ï¼‰ï¼›è¿˜æœ‰ä¸€äº›å®˜æ–¹æä¾›æµ‹è¯•çš„å›¾ç‰‡ã€‚å¦‚æœæ˜¯è®­ç»ƒè‡ªå·±çš„æ•°æ®é›†çš„è¯ï¼Œé‚£ä¹ˆå°±éœ€è¦ä¿®æ”¹å…¶ä¸­çš„yamlæ–‡ä»¶ã€‚ä½†æ˜¯è‡ªå·±çš„æ•°æ®é›†ä¸å»ºè®®æ”¾åœ¨è¿™ä¸ªè·¯å¾„ä¸‹é¢ï¼Œè€Œæ˜¯å»ºè®®æŠŠæ•°æ®é›†æ”¾åˆ°yolov5é¡¹ç›®çš„åŒçº§ç›®å½•ä¸‹é¢ã€‚

![](./legend/dataç»“æ„.png)

â”œâ”€â”€ modelsï¼šé‡Œé¢ä¸»è¦æ˜¯ä¸€äº›ç½‘ç»œæ„å»ºçš„é…ç½®æ–‡ä»¶å’Œå‡½æ•°ï¼Œå…¶ä¸­åŒ…å«äº†è¯¥é¡¹ç›®çš„å››ä¸ªä¸åŒçš„ç‰ˆæœ¬ï¼Œåˆ†åˆ«ä¸ºæ˜¯sã€mã€lã€xã€‚ä»åå­—å°±å¯ä»¥çœ‹å‡ºï¼Œè¿™å‡ ä¸ªç‰ˆæœ¬çš„å¤§å°ã€‚ä»–ä»¬çš„æ£€æµ‹æµ‹åº¦åˆ†åˆ«éƒ½æ˜¯ä»å¿«åˆ°æ…¢ï¼Œä½†æ˜¯ç²¾ç¡®åº¦åˆ†åˆ«æ˜¯ä»ä½åˆ°é«˜ã€‚è¿™å°±æ˜¯æ‰€è°“çš„é±¼å’Œç†ŠæŒä¸å¯å…¼å¾—ã€‚å¦‚æœè®­ç»ƒè‡ªå·±çš„æ•°æ®é›†çš„è¯ï¼Œå°±éœ€è¦ä¿®æ”¹è¿™é‡Œé¢ç›¸å¯¹åº”çš„yamlæ–‡ä»¶æ¥è®­ç»ƒè‡ªå·±æ¨¡å‹ã€‚

![](./legend/modelsç»“æ„.png)

â”œâ”€â”€ utilsï¼šå­˜æ”¾çš„æ˜¯å·¥å…·ç±»çš„å‡½æ•°ï¼Œé‡Œé¢æœ‰losså‡½æ•°ï¼Œmetricså‡½æ•°ï¼Œplotså‡½æ•°ç­‰ç­‰ã€‚

![](./legend/utilsç»“æ„.png)

â”œâ”€â”€ weightsï¼šæ”¾ç½®è®­ç»ƒå¥½çš„æƒé‡å‚æ•°ã€‚

- é‡Œé¢å­˜æ”¾äº†ä¸€ä¸ªdownload_weights.shï¼Œå¯ä»¥é€šè¿‡shå»ä¸‹è½½æƒé‡ã€‚

- ä¹Ÿå¯ä»¥æ‰‹åŠ¨å»ä¸‹è½½ï¼Œæƒé‡ä¸‹è½½åœ°å€ï¼šhttps://github.com/ultralytics/yolov5/releases/tag/v7.0

- ```bash
  https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt
  ```

â”œâ”€â”€ detect.pyï¼šåˆ©ç”¨è®­ç»ƒå¥½çš„æƒé‡å‚æ•°è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¯ä»¥è¿›è¡Œå›¾åƒã€è§†é¢‘å’Œæ‘„åƒå¤´çš„æ£€æµ‹ã€‚

â”œâ”€â”€ train.pyï¼šè®­ç»ƒè‡ªå·±çš„æ•°æ®é›†çš„å‡½æ•°ã€‚

â”œâ”€â”€ test.pyï¼šæµ‹è¯•è®­ç»ƒçš„ç»“æœçš„å‡½æ•°ã€‚

â”œâ”€â”€requirements.txtï¼šè¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œé‡Œé¢å†™ç€ä½¿ç”¨yolov5é¡¹ç›®çš„ç¯å¢ƒä¾èµ–åŒ…çš„ä¸€äº›ç‰ˆæœ¬ï¼Œå¯ä»¥åˆ©ç”¨è¯¥æ–‡æœ¬å¯¼å…¥ç›¸åº”ç‰ˆæœ¬çš„åŒ…ã€‚

## 0.2 [GPUç¯å¢ƒæ­å»º](https://blog.csdn.net/qq_53357019/article/details/125725702)

### 0.2.1 å®‰è£…nvidiaæ˜¾å¡é©±åŠ¨ã€cuda toolkitã€cudnn



**CUDA Toolkit** æ˜¯ç”¨äºå¼€å‘å’Œè¿è¡ŒåŸºäº CUDA çš„åº”ç”¨ç¨‹åºçš„è½¯ä»¶åŒ…ã€‚å®ƒåŒ…å«äº†ç¼–è¯‘å™¨ã€åº“ã€å·¥å…·å’Œç¤ºä¾‹ç­‰ç»„ä»¶ï¼Œç”¨äºç¼–å†™ã€æ„å»ºå’Œä¼˜åŒ– CUDA åº”ç”¨ç¨‹åºã€‚CUDA Toolkit è¿˜æä¾›äº†ä¸ GPU ç›¸å…³çš„é©±åŠ¨ç¨‹åºå’Œè¿è¡Œæ—¶åº“ï¼Œä»¥ä¾¿åœ¨ç³»ç»Ÿä¸Šæ­£ç¡®é…ç½®å’Œç®¡ç† GPUã€‚è¿™ä¸ªåº“çš„ä¸»è¦ç›®çš„æ˜¯å¸®ä½ å°è£…å¥½äº†å¾ˆå¤šçš„æ“ä½œè¿™ä¸ªgpu ï¼Œä¹Ÿå°±æ˜¯æ“ä½œè¿™ä¸ª cuda é©±åŠ¨çš„åº“ã€‚

**cuDNNï¼ˆCUDA Deep Neural Network libraryï¼‰**æ˜¯ NVIDIA ä¸ºæ·±åº¦å­¦ä¹ æ¡†æ¶æä¾›çš„åŠ é€Ÿåº“ã€‚å®ƒä¸ºæ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒå’Œæ¨ç†æä¾›äº†é«˜æ€§èƒ½çš„ GPU åŠ é€Ÿæ”¯æŒã€‚cuDNN æä¾›äº†ä¸€ç³»åˆ—ä¼˜åŒ–çš„ç®—æ³•å’Œå‡½æ•°ï¼Œç”¨äºåŠ é€Ÿå·ç§¯ã€æ± åŒ–ã€å½’ä¸€åŒ–ç­‰å¸¸ç”¨çš„æ·±åº¦å­¦ä¹ æ“ä½œã€‚å®ƒä¸ CUDA å’Œ CUDA Toolkit é…åˆä½¿ç”¨ï¼Œæä¾›äº†å¯¹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚TensorFlowã€PyTorchç­‰ï¼‰çš„ GPU åŠ é€Ÿèƒ½åŠ›ã€‚

[nvidia æ˜¾å¡é©±åŠ¨ å®‰è£…æœ€é¡ºçš„æ•™ç¨‹](https://zhuanlan.zhihu.com/p/302692454)ï¼Œæ¨èæŸ¥çœ‹

[é€‰æ‹©æ˜¾å¡é©±åŠ¨ç‰ˆæœ¬å’Œtoolkitç‰ˆæœ¬ä¸‹è½½ï¼Œä¸å«å®‰è£…æŠ¥é”™çš„æ˜¾å¡é©±åŠ¨å®‰è£…æ•™ç¨‹](https://blog.csdn.net/weixin_39928010/article/details/131142603)

[ubuntu cudnn å®‰è£…](https://blog.csdn.net/shanglianlm/article/details/130219640)

### 0.2.2 python ç¯å¢ƒå®‰è£…

[è§£å†³torchå®‰è£…ç¼“æ…¢å¤±è´¥åŠå…¶ä»–å®‰è£…åŒ…å¿«é€Ÿä¸‹è½½æ–¹æ³•](https://blog.csdn.net/qq_35207086/article/details/123482458)

```bash
# å®‰è£…æœ‰äº›åŒ…çš„æ—¶å€™ï¼Œå¾ˆæ…¢ï¼Œå¯ä»¥é€šè¿‡æ¸…åæºçš„æ–¹å¼ä¿®æ”¹
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple torch==2.0.1

#ï¼ˆ1ï¼‰é˜¿é‡Œäº‘     https://mirrors.aliyun.com/pypi/simple/
#ï¼ˆ2ï¼‰è±†ç“£     https://pypi.douban.com/simple/
#ï¼ˆ3ï¼‰æ¸…åå¤§å­¦     https://pypi.tuna.tsinghua.edu.cn/simple/
#ï¼ˆ4ï¼‰ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦     https://pypi.mirrors.ustc.edu.cn/simple/
#ï¼ˆ5ï¼‰åä¸­ç§‘æŠ€å¤§å­¦  https://pypi.hustunique.com/
```



```bash
# åˆ›å»ºä¸€ä¸ªæ²™ç®±ï¼Œpython å¤§äºç­‰äº3.8
conda create -n yolov5 python=3.10

conda activate yolov5
# ä¸‹è½½yolov5æºä»£ç åº“
git clone https://github.com/ultralytics/yolov5.git

cd yolov5

# 
pip install -r requirements.txt		# -Uå‚æ•°ä¸ç”¨æŒ‡å®š	
# -Uï¼š-U, --upgrade            Upgrade all specified packages to the newest available version. The handling of dependencies depends on the upgrade-strategy used.
# -r, --requirement <file>    Install from the given requirements file. This option can be used multiple times.

```



## 0.3 cocoæ•°æ®é›†

```bash
# coco
wget http://images.cocodataset.org/zips/train2017.zip	# 19G, 118k images
wget http://images.cocodataset.org/zips/val2017.zip		# 1G, 5k images
wget http://images.cocodataset.org/zips/test2017.zip	# 7G, 41k images
wget https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip	# æ•°æ®çš„æ ‡ç­¾ï¼Œè§£å‹ä¸Šé¢çš„å›¾ç‰‡åˆ°æ­¤labelæ–‡ä»¶å¤¹å†…ã€‚

# coco128ï¼Œä»train2017éšå³é€‰å–çš„128å¼ å›¾ç‰‡
https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip

# ä¸‹è½½yolov5å¯¹åº”ä»£ç çš„ç‰ˆæœ¬tagç‰ˆæœ¬ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ç”¨çš„æ—¶tag v5.0ç‰ˆæœ¬
https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.
https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5m.pt
https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5l.pt
https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5x.pt
```

# 1 å…¨æµç¨‹

## 1.1 æ ‡æ³¨æ•°æ®

éœ€è¦åœ¨æœ‰ç•Œé¢çš„ä¸»æœºä¸Šå®‰è£…ï¼Œè¿œç¨‹sshæ— æ³•ä½¿ç”¨çª—å£

```bash
# å®‰è£…
pip install labelImg
# å¯åŠ¨
labelImg
```

æ ‡æ³¨

![img](./legend/wp.jpeg)

![img](./legend/wp-1703317059823-3.jpeg)

- ä¸€å¼ å›¾ç‰‡å¯¹åº”ä¸€ä¸ªtxtæ ‡æ³¨æ–‡ä»¶ï¼ˆå¦‚æœå›¾ä¸­æ— æ‰€è¦ç‰©ä½“ï¼Œåˆ™æ— éœ€txtæ–‡ä»¶ï¼‰ï¼›
- txtæ¯è¡Œä¸€ä¸ªç‰©ä½“ï¼ˆä¸€å¼ å›¾ä¸­å¯ä»¥æœ‰å¤šä¸ªæ ‡æ³¨ï¼‰ï¼›
- æ¯è¡Œæ•°æ®æ ¼å¼ï¼š`ç±»åˆ«idã€x_center y_center width height`ï¼›
- **xywh**å¿…é¡»å½’ä¸€åŒ–ï¼ˆ0-1ï¼‰ï¼Œå…¶ä¸­`x_centerã€width`é™¤ä»¥å›¾ç‰‡å®½åº¦ï¼Œ`y_centerã€height`é™¤ä»¥ç”»é¢é«˜åº¦ï¼›
- ç±»åˆ«idå¿…é¡»ä»0å¼€å§‹è®¡æ•°ã€‚

## 1.2 å‡†å¤‡æ•°æ®é›†

### æ•°æ®é›†ç»“æ„ä¸å­˜æ”¾ä½ç½®

```bash
. å·¥ä½œè·¯å¾„
â”œâ”€â”€ datasets
â”‚   â””â”€â”€ person_data
â”‚       â”œâ”€â”€ images
â”‚       â”‚   â”œâ”€â”€ train
â”‚       â”‚   â”‚   â””â”€â”€ demo_001.jpg
â”‚       â”‚   â””â”€â”€ val
â”‚       â”‚       â””â”€â”€ demo_002.jpg
â”‚       â””â”€â”€ labels
â”‚           â”œâ”€â”€ train
â”‚           â”‚   â””â”€â”€ demo_001.txt
â”‚           â””â”€â”€ val
â”‚               â””â”€â”€ demo_002.txt
â””â”€â”€ yolov5
```

**è¦ç‚¹ï¼š**

- `datasets`ä¸`yolov5`åŒçº§ç›®å½•ï¼›
- å›¾ç‰‡ `datasets/person_data/images/train/{æ–‡ä»¶å}.jpg`å¯¹åº”çš„æ ‡æ³¨æ–‡ä»¶åœ¨ `datasets/person_data/labels/train/{æ–‡ä»¶å}.txt`ï¼ŒYOLOä¼šæ ¹æ®è¿™ä¸ªæ˜ å°„å…³ç³»è‡ªåŠ¨å¯»æ‰¾ï¼ˆ`images`æ¢æˆ`labels`ï¼‰ï¼›
- è®­ç»ƒé›†å’ŒéªŒè¯é›†
  - `images`æ–‡ä»¶å¤¹ä¸‹æœ‰`train`å’Œ`val`æ–‡ä»¶å¤¹ï¼Œåˆ†åˆ«æ”¾ç½®è®­ç»ƒé›†å’ŒéªŒè¯é›†å›¾ç‰‡;
  - `labels`æ–‡ä»¶å¤¹æœ‰`train`å’Œ`val`æ–‡ä»¶å¤¹ï¼Œåˆ†åˆ«æ”¾ç½®è®­ç»ƒé›†å’ŒéªŒè¯é›†æ ‡ç­¾(yoloæ ¼å¼ï¼‰;

###  åˆ›å»ºæ•°æ®é›†çš„é…ç½®æ–‡ä»¶

å¤åˆ¶`yolov5/data/coco128.yaml`ä¸€ä»½ï¼Œæ¯”å¦‚ä¸º`coco_person.yaml`

```yaml
# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ../datasets/person_data  # æ•°æ®æ‰€åœ¨ç›®å½•
train: images/train  # è®­ç»ƒé›†å›¾ç‰‡æ‰€åœ¨ä½ç½®ï¼ˆç›¸å¯¹äºpathï¼‰
val:  images/val # éªŒè¯é›†å›¾ç‰‡æ‰€åœ¨ä½ç½®ï¼ˆç›¸å¯¹äºpathï¼‰
test:  # æµ‹è¯•é›†å›¾ç‰‡æ‰€åœ¨ä½ç½®ï¼ˆç›¸å¯¹äºpathï¼‰ï¼ˆå¯é€‰ï¼‰

# ç±»åˆ«
nc: 5  # ç±»åˆ«æ•°é‡
names: ['pedestrians','riders','partially-visible-person','ignore-regions','crowd'] # ç±»åˆ«æ ‡ç­¾å
```

## 1.3 è®­ç»ƒ

```bash
# åˆ‡æ¢yolov5åˆ°æŒ‡å®šåˆ†æ”¯
git checkout a80dd66efe0bc7fe3772f259260d5b7278aab42f
# æŸ¥çœ‹å½“å‰ç‰ˆæœ¬
git log -1 --pretty=format:"%h"
```



### é€‰æ‹©å¹¶åˆ›å»ºæ¨¡å‹çš„é…ç½®æ–‡ä»¶

> å®˜æ–¹æƒé‡ä¸‹è½½åœ°å€ï¼šhttps://github.com/ultralytics/yolov5

![img](./legend/wp-1703317810676-6.jpeg)

æ ¹æ®ä½ çš„è®¾å¤‡ï¼Œé€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…·ä½“æ¨¡å‹æ¯”å¯¹å¦‚ä¸‹ï¼š

![img](./legend/wp-1703317810676-7.jpeg)

å¤åˆ¶`models`ä¸‹å¯¹åº”æ¨¡å‹çš„`yaml`æ–‡ä»¶ï¼Œé‡å‘½åï¼Œæ¯”å¦‚è¯¾ç¨‹å¦å­˜ä¸º`yolov5s_person.yaml`ï¼Œå¹¶ä¿®æ”¹å…¶ä¸­ï¼š

```shell
# nc: 80  # ç±»åˆ«æ•°é‡
nc: 5  # number of classes
```

### è®­ç»ƒ

ä¸‹è½½å¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œå¯ä»¥æ”¾åˆ°`weights`ç›®å½•ä¸‹ï¼Œè®¾ç½®æœ¬æœºæœ€å¥½æ€§èƒ½çš„å„ä¸ªå‚æ•°ï¼Œå³å¯å¼€å§‹è®­ç»ƒï¼Œè¯¾ç¨‹ä¸­è®­ç»ƒäº†ä»¥ä¸‹å‚æ•°ï¼š

```shell
# yolov5s 
python ./train.py --data ./data/coco_person.yaml --cfg ./models/yolov5s_person.yaml --weights ./weights/yolov5s.pt --batch-size 16 --epochs 120 --workers 0 --name s_120 --project yolo_person_s
```

> æ›´å¤šå‚æ•°è§`train.py`ï¼›
>
> è®­ç»ƒç»“æœåœ¨`yolo_person_s/`ä¸­å¯è§ï¼Œä¸€èˆ¬è®­ç»ƒæ—¶é—´åœ¨å‡ ä¸ªå°æ—¶ä»¥ä¸Šã€‚

å»ºè®®gpuå†…å­˜å°çš„æœºå™¨ï¼Œbatch-sizeé€‰æ‹©16åŠä»¥ä¸‹çš„æ•°å€¼ï¼ˆ4çš„å€æ•°ï¼‰ï¼Œè¿‡å¤§ä¼šå¯¼è‡´ç›¸å…³é—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­æ–­

å¦‚ä»¥ä¸‹ï¼š

1. RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED
2. torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 5.79 GiB total capacity; 4.79 GiB already allocated; 52.69 MiB free; 4.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
3. 

### è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–

#### wandb

YOLOå®˜ç½‘æ¨èä½¿ç”¨https://wandb.ai/ã€‚

- å»å®˜ç½‘æ³¨å†Œè´¦å·ï¼›
- è·å–`key`ç§˜é’¥ï¼Œåœ°å€ï¼šhttps://wandb.ai/authorize
- ä½¿ç”¨`pip install wandb`å®‰è£…åŒ…ï¼›
- ä½¿ç”¨`wandb login`ç²˜è´´ç§˜é’¥åç™»å½•ï¼›
- æ‰“å¼€ç½‘ç«™å³å¯æŸ¥çœ‹è®­ç»ƒè¿›å±•ã€‚

```bash
pip install wandb
```

![img](./legend/wp-1703381122005-12.jpeg)

#### tensorboard

```bash
tensorboard --logdir=./yolo_person_s
```

![img](./legend/wp-1703381182111-15.jpeg)



## 1.4 æµ‹è¯•ä¸è¯„ä¼°

### æµ‹è¯•

```bash
# å¦‚                                                         
python detect.py --source ./000057.jpg --weights ./yolo_person_s/s_120/weights/best.pt --conf-thres 0.3
# æˆ–
python detect.py --source ./c3.mp4 --weights ./yolo_person_s/s_120/weights/best.pt --conf-thres 0.3
```

### è¯„ä¼°

```bash
python val.py --data  ./data/coco_person.yaml  --weights ./yolo_person_s/s_120/weights/best.pt --batch-size 12

val: data=./data/coco_person.yaml, weights=['./yolo_person_s/s_1203/weights/best.pt'], batch_size=12, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False
YOLOv5 ğŸš€ v7.0-212-g9974d51 Python-3.9.17 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 5928MiB)

Fusing layers... 
YOLOv5s_person summary: 157 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs
val: Scanning /home/buntu/gitRepository/yoloXXX/datasets/person_data/labels/val.cache... 1000 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<?, ?it/s]
                 Class     Images  Instances          P          R      mAP50   mAP50-95:   2%|â–         | 2/84 [00:02<01:40,  1.22s/it]WARNING âš ï¸ NMS time limit 1.100s exceeded
                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:12<00:00,  6.58it/s]
                   all       1000      28423      0.508      0.366      0.379      0.205
           pedestrians       1000      17833      0.754      0.836      0.872      0.605
                riders       1000        185      0.609      0.492      0.495      0.237
partially-visible persons       1000       9335      0.509      0.322      0.341      0.124
        ignore regions       1000        409      0.428      0.144      0.154     0.0528
                 crowd       1000        661      0.242     0.0358     0.0312    0.00788
Speed: 0.1ms pre-process, 6.5ms inference, 3.3ms NMS per image at shape (12, 3, 640, 640)
Results saved to runs/val/exp2
```

## 1.5 å¯¼å‡ºonnx

```bash
pip install onnx

# å¦‚æœtorchæ˜¯GPUç‰ˆæœ¬ï¼Œå°±å¯ä»¥å®‰è£…onnxruntime-gpu
# æ£€æŸ¥torchæ˜¯ä»€ä¹ˆç‰ˆæœ¬ï¼Œè¿›å…¥pythonçš„å‘½ä»¤äº¤äº’å¼ç¯å¢ƒ
import torch
torch.__version__				# å¦‚æœæ˜¯2.0.1+cu117ï¼Œå°±æ˜¯GPUç‰ˆæœ¬
torch.cuda.is_available()		# æŸ¥çœ‹cudaæ˜¯å¦å¯ç”¨
torch.cuda.get_device_name(0)	# æŸ¥çœ‹gpuè®¾å¤‡åç§°

# å¦‚æœpytorchæ˜¯CPUç‰ˆæœ¬å°±ä¸èƒ½å®‰è£…onnxruntimeçš„gpuç‰ˆæœ¬
# GPUç‰ˆæœ¬
pip install onnxruntime-gpu==1.16    #ï¼ˆæœ¬äººcuda12.2.ï¼Œå®‰è£…1.16å¯ç”¨ï¼‰
# CPUç‰ˆæœ¬
pip install onnxruntime   

# éªŒè¯onnxruntime GPUç‰ˆæœ¬æ˜¯å¦å¯ç”¨
import onnxruntime
onnxruntime.get_device()
onnxruntime.get_available_providers()
```



åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`tensort decode plugin`æ¥ä»£æ›¿åŸæ¥yolov5ä»£ç ä¸­çš„decodeæ“ä½œï¼Œå¦‚æœä¸æ›¿æ¢ï¼Œè¿™éƒ¨åˆ†è¿ç®—å°†å½±å“æ•´ä½“æ€§èƒ½ã€‚

ä¸ºäº†è®©`tensorrt`èƒ½å¤Ÿè¯†åˆ«å¹¶åŠ è½½æˆ‘ä»¬é¢å¤–æ·»åŠ çš„`plugin operator`ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹Yolov5ä»£ç ä¸­å¯¼å‡ºonnxæ¨¡å‹çš„éƒ¨åˆ†ã€‚

<img src="./legend/wp-1703410251412-18.jpeg" alt="img" style="zoom: 33%;" />

### 1.5.1 ä¿®æ”¹yolov5 decodeéƒ¨åˆ†ä»£ç 

å°†è®­ç»ƒåçš„æ¨¡å‹ï¼ˆé¡¹ç›®å/s_xxx/weights/best.ptï¼‰ç§»åŠ¨é‡å‘½ååˆ°weights/yolov5s_person.pt

```bash
# ç”¨äºå¯¼å‡ºonnxæ—¶ï¼Œå¯¹æ¨¡å‹è¿›è¡Œç®€åŒ–
pip install onnx-simplifier  # >= 0.3.10
# ç”¨äºå¯è§†åŒ–onnxæ¨¡å‹ç»“æ„
pip install netron

# seabornæ˜¯pythonä¸­çš„ä¸€ä¸ªå¯è§†åŒ–åº“ï¼Œæ˜¯å¯¹matplotlibè¿›è¡ŒäºŒæ¬¡å°è£…è€Œæˆ
pip install seaborn
# onnxçš„æ‰‹æœ¯åˆ€å·¥å…·ï¼Œå¯æ”¹å˜ç½‘ç»œç»“æ„
pip install onnx-graphsurgeon

apt update
apt install -y libgl1-mesa-glx		# openglçš„å›¾å½¢ä¾èµ–åŒ…

# ä¿®æ”¹ä¹‹å‰ï¼Œå»ºè®®å…ˆä½¿ç”¨export.py å¯¼å‡ºä¸€ä»½åŸå§‹æ“ä½œçš„onnxæ¨¡å‹ï¼Œä»¥ä¾¿å’Œä¿®æ”¹åçš„æ¨¡å‹è¿›è¡Œå¯¹æ¯”ã€‚
python export.py --weights weights/yolov5s_person.pt --include onnx --simplify --dynamic
# å¯è§†åŒ–åŸå§‹çš„æ¨¡å‹
netron ./weights/yolov5s_person.onnx
```

```bash
# é€šè¿‡gitçš„è¡¥ä¸ï¼Œä¿®æ”¹export.py
git am export.patch
# å¯ä»¥ä¸‹æ¥ä»”ç»†ç ”ç©¶exportçš„å˜åŒ–

# ç„¶åå¯¼å‡ºä¿®æ”¹åçš„ç½‘ç»œ
python export.py --weights weights/yolov5s_person.pt --include onnx --simplify --dynamic
```

![](./legend/ä¿®æ”¹yolov5çš„decodeç½‘ç»œæ¨¡å—.png)

### 1.5.2 å…·ä½“ä¿®æ”¹ç»†èŠ‚

åœ¨`models/yolo.py`æ–‡ä»¶ä¸­54è¡Œï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹`class Detect`çš„forwardæ–¹æ³•ï¼Œä»¥åˆ é™¤å…¶box decodeè¿ç®—ï¼Œä»¥ç›´æ¥è¾“å‡ºç½‘ç»œç»“æœã€‚åœ¨åé¢çš„tensorrtéƒ¨ç½²ä¸­ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨decode pluginæ¥è¿›è¡Œdecodeæ“ä½œï¼Œå¹¶ç”¨gpuåŠ é€Ÿã€‚ä¿®æ”¹å†…å®¹å¦‚ä¸‹ï¼š

```python
-            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
-            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()
-
-            if not self.training:  # inference
-                if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:
-                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)
-
-                y = x[i].sigmoid()
-                if self.inplace:
-                    y[..., 0:2] = (y[..., 0:2] * 2 + self.grid[i]) * self.stride[i]  # xy
-                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
-                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953
-                    xy, wh, conf = y.split((2, 2, self.nc + 1), 4)  # y.tensor_split((2, 4, 5), 4)  # torch 1.8.0
-                    xy = (xy * 2 + self.grid[i]) * self.stride[i]  # xy
-                    wh = (wh * 2) ** 2 * self.anchor_grid[i]  # wh
-                    y = torch.cat((xy, wh, conf), 4)
-                z.append(y.view(bs, -1, self.no))
-
-        return x if self.training else (torch.cat(z, 1),) if self.export else (torch.cat(z, 1), x)
+            y = x[i].sigmoid()
+            z.append(y)
+        return z
```

å¯ä»¥çœ‹åˆ°è¿™é‡Œåˆ é™¤äº†ä¸»è¦çš„è¿ç®—éƒ¨åˆ†ï¼Œå°†æ¨¡å‹è¾“å‡ºç›´æ¥ä½œä¸ºlistè¿”å›ã€‚ä¿®æ”¹åï¼Œonnxçš„è¾“å‡ºå°†è¢«ä¿®æ”¹ä¸ºä¸‰ä¸ªåŸå§‹ç½‘ç»œè¾“å‡ºï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¾“å‡ºåæ·»åŠ decode pluginçš„ç®—å­ã€‚é¦–å…ˆæˆ‘ä»¬å…ˆå¯¼å‡ºonnxï¼Œå†åˆ©ç”¨nvidiaçš„graph surgeonæ¥ä¿®æ”¹onnxã€‚é¦–å…ˆæˆ‘ä»¬ä¿®æ”¹onnx exportéƒ¨åˆ†ä»£ç ï¼š

> GraphSurgeon æ˜¯nvidiaæä¾›çš„å·¥å…·ï¼Œå¯ä»¥æ–¹ä¾¿çš„ç”¨äºä¿®æ”¹ã€æ·»åŠ æˆ–è€…åˆ é™¤onnxç½‘ç»œå›¾ä¸­çš„èŠ‚ç‚¹ï¼Œå¹¶ç”Ÿæˆæ–°çš„onnxã€‚å‚è€ƒé“¾æ¥ï¼šhttps://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeonã€‚

```python
torch.onnx.export(
        model,
        im,
        f,
        verbose=False,
        opset_version=opset,
        training=torch.onnx.TrainingMode.TRAINING if train else torch.onnx.TrainingMode.EVAL,
        do_constant_folding=not train,
        input_names=['images'],
        output_names=['p3', 'p4', 'p5'],
        dynamic_axes={
            'images': {
                0: 'batch',
                2: 'height',
                3: 'width'},  # shape(1,3,640,640)
            'p3': {
                0: 'batch',
                2: 'height',
                3: 'width'},  # shape(1,25200,4)
            'p4': {
                0: 'batch',
                2: 'height',
                3: 'width'},
            'p5': {
                0: 'batch',
                2: 'height',
                3: 'width'}
        } if dynamic else None)
```

å°†onnxçš„è¾“å‡ºæ”¹ä¸º3ä¸ªåŸå§‹ç½‘ç»œè¾“å‡ºã€‚è¾“å‡ºå®Œæˆåï¼Œæˆ‘ä»¬å†åŠ è½½onnxï¼Œå¹¶simplifyï¼š

```python
model_onnx = onnx.load(f)
model_onnx = onnx.load(f)  # load onnx model
onnx.checker.check_model(model_onnx)  # check onnx model

# Simplify
if simplify:
    # try:
    check_requirements(('onnx-simplifier',))
    import onnxsim

    LOGGER.info(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')
    model_onnx, check = onnxsim.simplify(model_onnx,
        dynamic_input_shape=dynamic,
        input_shapes={'images': list(im.shape)} if dynamic else None)
    assert check, 'assert check failed'
    onnx.save(model_onnx, f)
```

ç„¶åæˆ‘ä»¬å†å°†onnxåŠ è½½å›æ¥ï¼Œç”¨nvidia surgeonè¿›è¡Œä¿®æ”¹:

```python
import onnx_graphsurgeon as onnx_gs
import numpy as np
yolo_graph = onnx_gs.import_onnx(model_onnx)
```

é¦–å…ˆæˆ‘ä»¬è·å–åŸå§‹çš„onnxè¾“å‡ºp3,p4,p5ï¼š

```python
p3 = yolo_graph.outputs[0]
p4 = yolo_graph.outputs[1]
p5 = yolo_graph.outputs[2]
```

ç„¶åæˆ‘ä»¬å®šä¹‰æ–°çš„onnxè¾“å‡ºï¼Œç”±äºdecode pluginä¸­ï¼Œæœ‰4ä¸ªè¾“å‡ºï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å®šä¹‰4ä¸ªæ–°çš„è¾“å‡ºã€‚å…¶åå­—éœ€è¦å’Œä¸‹é¢çš„ä»£ç ä¿æŒä¸€è‡´ï¼Œè¿™æ˜¯decode_pluginä¸­é¢„å…ˆå®šä¹‰å¥½çš„ã€‚

```python
decode_out_0 = onnx_gs.Variable(
  "DecodeNumDetection",
  dtype=np.int32
)
decode_out_1 = onnx_gs.Variable(
  "DecodeDetectionBoxes",
  dtype=np.float32
)
decode_out_2 = onnx_gs.Variable(
  "DecodeDetectionScores",
  dtype=np.float32
)
decode_out_3 = onnx_gs.Variable(
  "DecodeDetectionClasses",
  dtype=np.int32
)
```

ç„¶åæˆ‘ä»¬éœ€è¦å†æ·»åŠ ä¸€äº›decodeå‚æ•°ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š

```python
decode_attrs = dict()

decode_attrs["max_stride"] = int(max(model.stride))
decode_attrs["num_classes"] = model.model[-1].nc
decode_attrs["anchors"] = [float(v) for v in [10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326]]
decode_attrs["prenms_score_threshold"] = 0.25
```

åœ¨å®šä¹‰å¥½äº†ç›¸å…³å‚æ•°åï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªonnx nodeï¼Œç”¨ä½œdecode pluginã€‚ç”±äºæˆ‘ä»¬çš„tensorrt pluginçš„åç§°ä¸º`YoloLayer_TRT`,å› æ­¤è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿æŒopçš„åå­—ä¸æˆ‘ä»¬çš„pluginåç§°ä¸€è‡´ã€‚é€šè¿‡å¦‚ä¸‹ä»£ç ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªnodeï¼š

```python
decode_plugin = onnx_gs.Node(
        op="YoloLayer_TRT",
        name="YoloLayer",
        inputs=[p3, p4, p5],
        outputs=[decode_out_0, decode_out_1, decode_out_2, decode_out_3],
        attrs=decode_attrs
    )
```

ç„¶åæˆ‘ä»¬å°†è¿™ä¸ªnodeæ·»åŠ äº†ç½‘ç»œä¸­ï¼š

```python
yolo_graph.nodes.append(decode_plugin)
    yolo_graph.outputs = decode_plugin.outputs
    yolo_graph.cleanup().toposort()
    model_onnx = onnx_gs.export_onnx(yolo_graph)
```

æœ€åæ·»åŠ ä¸€äº›metaä¿¡æ¯åï¼Œæˆ‘ä»¬å¯¼å‡ºæœ€ç»ˆçš„onnxæ–‡ä»¶ï¼Œè¿™ä¸ªæ–‡ä»¶å¯ä»¥ç”¨äºåç»­çš„tensorrtéƒ¨ç½²å’Œæ¨ç†ã€‚

```python
d = {'stride': int(max(model.stride)), 'names': model.names}
    for k, v in d.items():
        meta = model_onnx.metadata_props.add()
        meta.key, meta.value = k, str(v)

    onnx.save(model_onnx, f)
    LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')
    return f
```

## 1.6 TensorRTéƒ¨ç½²

### 1.6.1 æ¨¡å‹æ„å»º 

```c++
#include "NvInfer.h"
#include "NvOnnxParser.h" // onnxparserå¤´æ–‡ä»¶
#include "logger.h"
#include "common.h"
#include "buffers.h"
#include "cassert"
#include "./utils/common_utils.h"

// mainå‡½æ•°
int main(int argc, char **argv)
{
    if (argc != 3)
    {
        std::cerr << "ç”¨æ³•: ./build [input_onnx_file_name] [output_file_name]" << std::endl;
        return -1;
    }
    char cwd[128] = {0};
    utils::getExeWd(cwd,128);


    // å‘½ä»¤è¡Œè·å–onnxæ–‡ä»¶è·¯å¾„
    std::string onnx_file_path = std::string(cwd) + "/weights/" + argv[1];
    std::string engine_file_path = std::string(cwd) + "/weights/" + argv[2];
    std::cout<< "onnx_file_pathï¼š"<< onnx_file_path << std::endl;
    std::cout<< "engine_file_pathï¼š"<< engine_file_path << std::endl;

    // =========== 1. åˆ›å»ºbuilder ===========
    auto builder = std::unique_ptr<nvinfer1::IBuilder>(nvinfer1::createInferBuilder(sample::gLogger.getTRTLogger()));
    if (!builder)
    {
        std::cerr << "Failed to create builder" << std::endl;
        return -1;
    }

    // ========== 2. åˆ›å»ºnetworkï¼šbuilder--->network ==========
    // æ˜¾æ€§batch
    const auto explicitBatch = 1U << static_cast<uint32_t>(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);
    // è°ƒç”¨builderçš„createNetworkV2æ–¹æ³•åˆ›å»ºnetwork
    auto network = std::unique_ptr<nvinfer1::INetworkDefinition>(builder->createNetworkV2(explicitBatch));
    if (!network)
    {
        std::cout << "Failed to create network" << std::endl;
        return -1;
    }
    // ä¸ä¸ŠèŠ‚è¯¾æ‰‹åŠ¨åˆ›å»ºç½‘ç»œä¸åŒï¼Œè¿™æ¬¡ä½¿ç”¨onnxparseråˆ›å»ºç½‘ç»œ

    // åˆ›å»ºonnxparserï¼Œç”¨äºè§£æonnxæ–‡ä»¶
    auto parser = std::unique_ptr<nvonnxparser::IParser>(nvonnxparser::createParser(*network, sample::gLogger.getTRTLogger()));
    // è°ƒç”¨onnxparserçš„parseFromFileæ–¹æ³•è§£æonnxæ–‡ä»¶
    auto parsed = parser->parseFromFile(onnx_file_path.c_str(), static_cast<int>(sample::gLogger.getReportableSeverity()));
    if (!parsed)
    {
        std::cout << "Failed to parse onnx file" << std::endl;
        return -1;
    }
    // é…ç½®ç½‘ç»œå‚æ•°
    // æˆ‘ä»¬éœ€è¦å‘Šè¯‰tensorrtæˆ‘ä»¬æœ€ç»ˆè¿è¡Œæ—¶ï¼Œè¾“å…¥å›¾åƒçš„èŒƒå›´ï¼Œbatch sizeçš„èŒƒå›´ã€‚è¿™æ ·tensorrtæ‰èƒ½å¯¹åº”ä¸ºæˆ‘ä»¬è¿›è¡Œæ¨¡å‹æ„å»ºä¸ä¼˜åŒ–ã€‚
    auto input = network->getInput(0);                                                                             // è·å–è¾“å…¥èŠ‚ç‚¹
    auto profile = builder->createOptimizationProfile();                                                           // åˆ›å»ºprofileï¼Œç”¨äºè®¾ç½®è¾“å…¥çš„åŠ¨æ€å°ºå¯¸
    profile->setDimensions(input->getName(), nvinfer1::OptProfileSelector::kMIN, nvinfer1::Dims4{1, 3, 640, 640}); // è®¾ç½®æœ€å°å°ºå¯¸
    profile->setDimensions(input->getName(), nvinfer1::OptProfileSelector::kOPT, nvinfer1::Dims4{1, 3, 640, 640}); // è®¾ç½®æœ€ä¼˜å°ºå¯¸
    profile->setDimensions(input->getName(), nvinfer1::OptProfileSelector::kMAX, nvinfer1::Dims4{1, 3, 640, 640}); // è®¾ç½®æœ€å¤§å°ºå¯¸

    // ========== 3. åˆ›å»ºconfigé…ç½®ï¼šbuilder--->config ==========
    auto config = std::unique_ptr<nvinfer1::IBuilderConfig>(builder->createBuilderConfig());
    if (!config)
    {
        std::cout << "Failed to create config" << std::endl;
        return -1;
    }
    // ä½¿ç”¨addOptimizationProfileæ–¹æ³•æ·»åŠ profileï¼Œç”¨äºè®¾ç½®è¾“å…¥çš„åŠ¨æ€å°ºå¯¸
    config->addOptimizationProfile(profile);

    // è®¾ç½®ç²¾åº¦ï¼Œä¸è®¾ç½®æ˜¯FP32ï¼Œè®¾ç½®ä¸ºFP16ï¼Œè®¾ç½®ä¸ºINT8éœ€è¦é¢å¤–è®¾ç½®calibrator 
    config->setFlag(nvinfer1::BuilderFlag::kFP16);
    // è®¾ç½®æœ€å¤§batchsize
    builder->setMaxBatchSize(1);
    // è®¾ç½®æœ€å¤§å·¥ä½œç©ºé—´ï¼ˆæ–°ç‰ˆæœ¬çš„TensorRTå·²ç»åºŸå¼ƒäº†setWorkspaceSizeï¼‰
    config->setMemoryPoolLimit(nvinfer1::MemoryPoolType::kWORKSPACE, 1 << 30);

    // åˆ›å»ºæµï¼Œç”¨äºè®¾ç½®profile
    auto profileStream = samplesCommon::makeCudaStream();
    if (!profileStream)
    {
        return -1;
    }
    config->setProfileStream(*profileStream);

    // ========== 4. åˆ›å»ºengineï¼šbuilder--->engine(*nework, *config) ==========
    // ä½¿ç”¨buildSerializedNetworkæ–¹æ³•åˆ›å»ºengineï¼Œå¯ç›´æ¥è¿”å›åºåˆ—åŒ–çš„engineï¼ˆåŸæ¥çš„buildEngineWithConfigæ–¹æ³•å·²ç»åºŸå¼ƒï¼Œéœ€è¦å…ˆåˆ›å»ºengineï¼Œå†åºåˆ—åŒ–ï¼‰
    auto plan = std::unique_ptr<nvinfer1::IHostMemory>(builder->buildSerializedNetwork(*network, *config));
    if (!plan)
    {
        std::cout << "Failed to create engine" << std::endl;
        return -1;
    }

    // ========== 5. åºåˆ—åŒ–ä¿å­˜engine ==========

    std::ofstream engine_file(engine_file_path, std::ios::binary);
    assert(engine_file.is_open() && "Failed to open engine file");
    engine_file.write((char *)plan->data(), plan->size());
    engine_file.close();

    // ========== 6. é‡Šæ”¾èµ„æº ==========
    // å› ä¸ºä½¿ç”¨äº†æ™ºèƒ½æŒ‡é’ˆï¼Œæ‰€ä»¥ä¸éœ€è¦æ‰‹åŠ¨é‡Šæ”¾èµ„æº

    std::cout << "Engine build success!" << std::endl;

    return 0;
}
```



### 1.6.2 è¿è¡Œæ—¶æ„å»º

```c++
#include "NvInfer.h"
#include "NvOnnxParser.h"
#include "logger.h"
#include "common.h"
#include "buffers.h"
#include "utils/preprocess.h"
#include "utils/postprocess.h"
#include "utils/types.h"
#include "./utils/common_utils.h"

// åŠ è½½æ¨¡å‹æ–‡ä»¶
std::vector<unsigned char> load_engine_file(const std::string &file_name)
{
    std::vector<unsigned char> engine_data;
    std::ifstream engine_file(file_name, std::ios::binary);
    assert(engine_file.is_open() && "Unable to load engine file.");
    engine_file.seekg(0, engine_file.end);
    int length = engine_file.tellg();
    engine_data.resize(length);
    engine_file.seekg(0, engine_file.beg);
    engine_file.read(reinterpret_cast<char *>(engine_data.data()), length);
    return engine_data;
}

int main(int argc, char **argv)
{
    if (argc < 3)
    {
        std::cerr << "ç”¨æ³•: " << argv[0] << " <engine_file> <input_path_path>" << std::endl;
        return -1;
    }
    char cwd[128] = {0};
    utils::getExeWd(cwd,128);

    auto engine_file = std::string(cwd) + "/weights/" + argv[1];      // æ¨¡å‹æ–‡ä»¶
    auto input_video_path = std::string(cwd) + "/media/" + argv[2]; // è¾“å…¥è§†é¢‘æ–‡ä»¶
    auto output_video_path = std::string(cwd) + "/media/" + std::to_string(utils::timeu::getSecTimeStamp()) + ".mp4";
    std::cout<< "engine_file: "<< engine_file << std::endl;
    std::cout<< "input_video_pathï¼š"<< input_video_path << std::endl;
    std::cout<< "output_video_pathï¼š"<< output_video_path << std::endl;


    // ========= 1. åˆ›å»ºæ¨ç†è¿è¡Œæ—¶runtime =========
    auto runtime = std::unique_ptr<nvinfer1::IRuntime>(nvinfer1::createInferRuntime(sample::gLogger.getTRTLogger()));
    if (!runtime)
    {
        std::cout << "runtime create failed" << std::endl;
        return -1;
    }
    // ======== 2. ååºåˆ—åŒ–ç”Ÿæˆengine =========
    // åŠ è½½æ¨¡å‹æ–‡ä»¶
    auto plan = load_engine_file(engine_file);
    // ååºåˆ—åŒ–ç”Ÿæˆengine
    auto mEngine = std::shared_ptr<nvinfer1::ICudaEngine>(runtime->deserializeCudaEngine(plan.data(), plan.size()));
    if (!mEngine)
    {
        return -1;
    }

    // ======== 3. åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡context =========
    auto context = std::unique_ptr<nvinfer1::IExecutionContext>(mEngine->createExecutionContext());
    if (!context)
    {
        std::cout << "context create failed" << std::endl;
        return -1;
    }

    // ========== 4. åˆ›å»ºè¾“å…¥è¾“å‡ºç¼“å†²åŒº =========
    samplesCommon::BufferManager buffers(mEngine);

    auto cap = cv::VideoCapture(input_video_path);

    int width = int(cap.get(cv::CAP_PROP_FRAME_WIDTH));
    int height = int(cap.get(cv::CAP_PROP_FRAME_HEIGHT));
    int fps = int(cap.get(cv::CAP_PROP_FPS));

    // å†™å…¥MP4æ–‡ä»¶ï¼Œå‚æ•°åˆ†åˆ«æ˜¯ï¼šæ–‡ä»¶åï¼Œç¼–ç æ ¼å¼ï¼Œå¸§ç‡ï¼Œå¸§å¤§å°
    cv::VideoWriter writer(output_video_path, cv::VideoWriter::fourcc('H', '2', '6', '4'), fps, cv::Size(width, height));

    cv::Mat frame;
    int frame_index{0};
    // ç”³è¯·gpuå†…å­˜
    cuda_preprocess_init(height * width);

    while (cap.isOpened())
    {
        // ç»Ÿè®¡è¿è¡Œæ—¶é—´
        auto start = std::chrono::high_resolution_clock::now();

        cap >> frame;
        if (frame.empty())
        {
            std::cout << "æ–‡ä»¶å¤„ç†å®Œæ¯•" << std::endl;
            break;
        }
        frame_index++;

        // è¾“å…¥é¢„å¤„ç†ï¼ˆå®ç°äº†å¯¹è¾“å…¥å›¾åƒå¤„ç†çš„gpu åŠ é€Ÿ)
        process_input(frame, (float *)buffers.getDeviceBuffer(kInputTensorName));
        // ========== 5. æ‰§è¡Œæ¨ç† =========
        context->executeV2(buffers.getDeviceBindings().data());
        // æ‹·è´å›host
        buffers.copyOutputToHost();

        // ä»buffer managerä¸­è·å–æ¨¡å‹è¾“å‡º
        int32_t *num_det = (int32_t *)buffers.getHostBuffer(kOutNumDet); // æ£€æµ‹åˆ°çš„ç›®æ ‡ä¸ªæ•°
        int32_t *cls = (int32_t *)buffers.getHostBuffer(kOutDetCls);     // æ£€æµ‹åˆ°çš„ç›®æ ‡ç±»åˆ«
        float *conf = (float *)buffers.getHostBuffer(kOutDetScores);     // æ£€æµ‹åˆ°çš„ç›®æ ‡ç½®ä¿¡åº¦
        float *bbox = (float *)buffers.getHostBuffer(kOutDetBBoxes);     // æ£€æµ‹åˆ°çš„ç›®æ ‡æ¡†
        // æ‰§è¡Œnmsï¼ˆéæå¤§å€¼æŠ‘åˆ¶ï¼‰ï¼Œå¾—åˆ°æœ€åçš„æ£€æµ‹æ¡†
        std::vector<Detection> bboxs;
        yolo_nms(bboxs, num_det, cls, conf, bbox, kConfThresh, kNmsThresh);

        // ç»“æŸæ—¶é—´
        auto end = std::chrono::high_resolution_clock::now();
        auto elapsed = std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();
        auto time_str = std::to_string(elapsed) + "ms";
        auto fps_str = std::to_string(1000 / elapsed) + "fps";

        // éå†æ£€æµ‹ç»“æœ
        for (size_t j = 0; j < bboxs.size(); j++)
        {
            cv::Rect r = get_rect(frame, bboxs[j].bbox);
            cv::rectangle(frame, r, cv::Scalar(0x27, 0xC1, 0x36), 2);
            cv::putText(frame, std::to_string((int)bboxs[j].class_id), cv::Point(r.x, r.y - 10), cv::FONT_HERSHEY_PLAIN, 1.2, cv::Scalar(0x27, 0xC1, 0x36), 2);
        }
        cv::putText(frame, time_str, cv::Point(50, 50), cv::FONT_HERSHEY_PLAIN, 1.2, cv::Scalar(0xFF, 0xFF, 0xFF), 2);
        cv::putText(frame, fps_str, cv::Point(50, 100), cv::FONT_HERSHEY_PLAIN, 1.2, cv::Scalar(0xFF, 0xFF, 0xFF), 2);

        // cv::imshow("frame", frame);
        // å†™å…¥è§†é¢‘æ–‡ä»¶
        writer.write(frame);
        std::cout << "å¤„ç†å®Œç¬¬" << frame_index << "å¸§" << std::endl;
        if (cv::waitKey(1) == 27)
            break;
    }
    // ========== 6. é‡Šæ”¾èµ„æº =========
    // å› ä¸ºä½¿ç”¨äº†unique_ptrï¼Œæ‰€ä»¥ä¸éœ€è¦æ‰‹åŠ¨é‡Šæ”¾

    return 0;
}

```

### 1.6.3 é‡åŒ–

æ·±åº¦å­¦ä¹ é‡åŒ–å°±æ˜¯å°†æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„å‚æ•°ï¼ˆä¾‹å¦‚æƒé‡å’Œåç½®ï¼‰ä»æµ®ç‚¹æ•°è½¬æ¢æˆæ•´æ•°æˆ–è€…å®šç‚¹æ•°çš„è¿‡ç¨‹ã€‚è¿™æ ·åšå¯ä»¥å‡å°‘æ¨¡å‹çš„å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼Œä»è€Œè¾¾åˆ°æ¨¡å‹å‹ç¼©å’Œè¿ç®—åŠ é€Ÿçš„ç›®çš„ã€‚å¦‚int8é‡åŒ–ï¼Œè®©åŸæ¥æ¨¡å‹ä¸­32bitå­˜å‚¨çš„æ•°å­—**æ˜ å°„**åˆ°8bitå†è®¡ç®—ï¼ˆèŒƒå›´æ˜¯[-128,127]ï¼‰ã€‚

- åŠ å¿«æ¨ç†é€Ÿåº¦ï¼šè®¿é—®ä¸€æ¬¡32ä½æµ®ç‚¹å‹å¯ä»¥è®¿é—®4æ¬¡int8æ•´å‹æ•°æ®ï¼›
- å‡å°‘å­˜å‚¨ç©ºé—´å’Œå†…å­˜å ç”¨ï¼šåœ¨è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚åµŒå…¥å¼ï¼‰ä¸Šéƒ¨ç½²æ›´å®ç”¨ã€‚

å½“ç„¶ï¼Œæå‡é€Ÿåº¦çš„åŒæ—¶ï¼Œé‡åŒ–ä¹Ÿä¼šå¸¦æ¥**ç²¾åº¦çš„æŸå¤±**ï¼Œä¸ºäº†èƒ½å°½å¯èƒ½å‡å°‘é‡åŒ–è¿‡ç¨‹ä¸­ç²¾åº¦çš„æŸå¤±ï¼Œéœ€è¦ä½¿ç”¨å„ç§æ ¡å‡†æ–¹æ³•æ¥é™ä½ä¿¡æ¯çš„æŸå¤±ã€‚TensorRT ä¸­æ”¯æŒä¸¤ç§ INT8 æ ¡å‡†ç®—æ³•ï¼š

- ç†µæ ¡å‡† (Entropy Calibration)
- æœ€å°æœ€å¤§å€¼æ ¡å‡† (Min-Max Calibration)

> ç†µæ ¡å‡†æ˜¯ä¸€ç§åŠ¨æ€æ ¡å‡†ç®—æ³•ï¼Œå®ƒä½¿ç”¨ KL æ•£åº¦ (KL Divergence) æ¥åº¦é‡æ¨ç†æ•°æ®å’Œæ ¡å‡†æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚åœ¨ç†µæ ¡å‡†ä¸­ï¼Œæ ¡å‡†æ•°æ®æ˜¯ä»å®æ—¶æ¨ç†æ•°æ®ä¸­é‡‡é›†çš„ï¼Œå®ƒå°† INT8 ç²¾åº¦é‡åŒ–å‚æ•°çœ‹ä½œæ¦‚ç‡åˆ†å¸ƒï¼Œæ ¹æ®æ¨ç†æ•°æ®å’Œæ ¡å‡†æ•°æ®çš„ KL æ•£åº¦æ¥æ›´æ–°é‡åŒ–å‚æ•°ã€‚è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯å¯ä»¥æ›´å¥½åœ°åæ˜ å®é™…æ¨ç†æ•°æ®çš„åˆ†å¸ƒã€‚
>
> æœ€å°æœ€å¤§å€¼æ ¡å‡†ä½¿ç”¨æœ€å°æœ€å¤§å€¼ç®—æ³•æ¥è®¡ç®—é‡åŒ–å‚æ•°ã€‚åœ¨æœ€å°æœ€å¤§å€¼æ ¡å‡†ä¸­ï¼Œéœ€è¦ä½¿ç”¨ä¸€ç»„ä»£è¡¨æ€§çš„æ ¡å‡†æ•°æ®æ¥ç”Ÿæˆé‡åŒ–å‚æ•°ï¼Œé¦–å…ˆå°†æ¨ç†ä¸­çš„æ•°æ®è¿›è¡Œç»Ÿè®¡ï¼Œè®¡ç®—æ•°æ®çš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œç„¶åæ ¹æ®è¿™äº›å€¼æ¥è®¡ç®—é‡åŒ–å‚æ•°ã€‚

è¿™ä¸¤ç§æ ¡å‡†æ–¹æ³•éƒ½éœ€è¦å‡†å¤‡ä¸€äº›æ•°æ®ç”¨äºåœ¨æ ¡å‡†æ—¶æ‰§è¡Œæ¨ç†ï¼Œä»¥ç»Ÿè®¡æ•°æ®çš„åˆ†å¸ƒæƒ…å†µã€‚**ä¸€èˆ¬æ•°æ®éœ€è¦æœ‰ä»£è¡¨æ€§ï¼Œå³éœ€è¦ç¬¦åˆæœ€ç»ˆå®é™…è½åœ°åœºæ™¯çš„æ•°æ®ã€‚**å®é™…åº”ç”¨ä¸­ä¸€èˆ¬å‡†å¤‡500-1000ä¸ªæ•°æ®ç”¨äºé‡åŒ–ã€‚

#### TensorRTä¸­å®ç°

åœ¨ TensorRT ä¸­ï¼Œå¯ä»¥é€šè¿‡å®ç° `IInt8EntropyCalibrator2` æ¥å£æˆ– `IInt8MinMaxCalibrator` æ¥å£æ¥æ‰§è¡Œç†µæ ¡å‡†æˆ–æœ€å°æœ€å¤§å€¼æ ¡å‡†ï¼Œå¹¶ä¸”éœ€è¦å®ç°å‡ ä¸ªè™šå‡½æ•°æ–¹æ³•ï¼š

- `getBatch() `æ–¹æ³•ï¼šç”¨äºæä¾›ä¸€æ‰¹æ ¡å‡†æ•°æ®ï¼›
- `readCalibrationCache()` å’Œ `writeCalibrationCache()` æ–¹æ³•ï¼šå®ç°ç¼“å­˜æœºåˆ¶ï¼Œä»¥é¿å…åœ¨æ¯æ¬¡å¯åŠ¨æ—¶é‡æ–°åŠ è½½æ ¡å‡†æ•°æ®ã€‚

##### æ„é€ æ¨ç†æ•°æ®

```bash
# è¿›å…¥mediaç›®å½•ï¼Œåœ¨è§†é¢‘ä¸­éšæœºæŒ‘é€‰200å¸§ç”»é¢ä½œä¸ºæ ¡å‡†å›¾ç‰‡
# ä»è§†é¢‘ä¸­ç”Ÿæˆå›¾ç‰‡
ffmpeg -i c3.mp4 sample%04d.png
# ä»ç”Ÿæˆçš„å›¾ç‰‡ä¸­æŒ‘é€‰200å¼ ï¼Œå¹¶å°†åå­—å†™å…¥txtä¸­
ls *.png | shuf -n 200 > filelist.txt
```

### 1.6.4 é¢„å¤„ç†

Yolov5å›¾åƒé¢„å¤„ç†æ­¥éª¤ä¸»è¦å¦‚ä¸‹ï¼š

1. **lettorbox**ï¼šå³ä¿æŒåŸå›¾æ¯”ä¾‹ï¼ˆå›¾åƒç›´æ¥resizeåˆ°è¾“å…¥å¤§å°æ•ˆæœä¸å¥½ï¼‰ï¼Œå°†å›¾ç‰‡æ”¾åœ¨ä¸€ä¸ªæ­£æ–¹å½¢çš„ç”»å¸ƒä¸­ï¼Œå¤šä½™çš„éƒ¨åˆ†ç”¨é»‘è‰²å¡«å……ã€‚
2. **Normalizationï¼ˆå½’ä¸€åŒ–ï¼‰**ï¼šå°†åƒç´ å€¼ç¼©æ”¾è‡³`[0,1]`é—´ï¼›
3. **é¢œè‰²é€šé“é¡ºåºè°ƒæ•´**ï¼šBGR2RGB
4. **NHWC è½¬ä¸º NCHW**



### 1.6.5 åå¤„ç†

éæå¤§å€¼æŠ‘åˆ¶

# log

1. [è¿è¡Œyolov5-5.0å‡ºç°AttributeError: Canâ€˜t get attribute â€˜SPPFâ€˜ æ­£ç¡®è§£å†³æ–¹æ³•](https://blog.csdn.net/qq_41035097/article/details/122884652)

   - weighté¢„ç½®æƒé‡ç‰ˆæœ¬å’Œå®é™…yolov5çš„tagä¸åŒ¹é…
   - ä¸èƒ½ç”¨weight 7.0 ç»™yolov5 tagv5.0æ¥è®­ç»ƒ

2. [AttributeError: module numpy has no attribute int .æŠ¥é”™è§£å†³æ–¹æ¡ˆ](https://blog.csdn.net/weixin_46669612/article/details/129624331)

   - å®˜æ–¹ç»™å‡ºçš„numpyçš„ç‰ˆæœ¬è¦æ±‚æ—¶>=1.18.5ï¼Œè€Œ[numpy](https://so.csdn.net/so/search?q=numpy&spm=1001.2101.3001.7020).intåœ¨[NumPy](https://so.csdn.net/so/search?q=NumPy&spm=1001.2101.3001.7020) 1.20ä¸­å·²å¼ƒç”¨ï¼Œåœ¨NumPy 1.24ä¸­å·²åˆ é™¤ã€‚
   - é‡è£…numpyï¼špip install numpy==1.22

3. [RuntimeError: result type Float canâ€˜t be cast to the desired output type long int](https://blog.csdn.net/bu_fo/article/details/130336910)

   ```python
   # loss.pyå‡ºé—®é¢˜
   indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))  # image, anchor, grid indices
   # è§£å†³
   indices.append((b, a, gj.clamp_(0, gain[3].long() - 1), gi.clamp_(0, gain[2].long() - 1)))  # image, anchor, grid indices
   ```

4. åœ¨è¿è¡Œæ‰§è¡Œbuild engineæ—¶ï¼ŒæŠ¥Error Code 6: Internal Error (Unable to load library: libnvinfer_builder_resource.so.8.6.1ï¼Œ[å¯ä»¥å‚è€ƒè‡ªå·±çš„æ–‡ç« ](https://blog.csdn.net/qq_42190134/article/details/135339907)

   - `sudo cp libnvinfer_builder_resource.so.8.6.1 /usr/lib/`

5. æ ¹æ®GPUå‰©ä½™å†…å­˜é€‰æ‹©GPUè®¾å¤‡

   - ä¸€å®šè¦è®°å¾—åœ¨cudaSetDeviceä¹‹åï¼ŒcudaDeviceResetä¸€ä¸‹ï¼Œå¦åˆ™æ¯ä¸ªè®¾å¤‡éƒ½ä¼šç•™ä¸‹ä¸Šä¸‹æ–‡å†…å­˜å ç”¨ï¼ˆä¸€ä¸ªè¿›ç¨‹è¿è¡Œåœ¨å¤šä¸ªgpuä¸Šï¼‰
   - ![](./legend/åŒä¸€è¿›ç¨‹setDeviceå¤šä¸ªgpuè®¾å¤‡ååœ¨æ‰€æœ‰è®¾å¤‡ä¸Šå æœ‰ä¸Šä¸‹æ–‡å†…å­˜.png)

   ```c++
   bool YOLOV8::chooseGPUDeviceWithMemory(int GpuMemoryUsage) {
       int deviceCount = 0;
       cudaGetDeviceCount(&deviceCount);
       if(deviceCount == 0){
           logger.error("å½“å‰æ²¡æœ‰å¯ç”¨çš„GPUè®¾å¤‡");
           return false;
       }else{
           std::string deviceCountInfo = std::string("å½“å‰æœ‰" + deviceCount) + "ä¸ªGPUè®¾å¤‡";
           logger.info(deviceCountInfo);
           std::cout<< "å½“å‰æœ‰" << deviceCount<< "ä¸ªGPUè®¾å¤‡" <<std::endl;
       }
   
       // éå†è®¾å¤‡ç¼–å·ä¿¡æ¯
       int device;
       int maxRestMemoryDevice = -1;
       double maxRestMemory = GpuMemoryUsage;
       size_t avail(0);//å¯ç”¨æ˜¾å­˜
       size_t total(0);//æ€»æ˜¾å­˜
       for (device = 0; device < deviceCount; ++device) {
           // setDeviceä¼šåœ¨æ¯ä¸ªgpuä¸Šåˆ›å»ºä¸€ä¸ªä¸Šä¸‹æ–‡ï¼Œå¦‚æœä¸æ‰‹åŠ¨é‡Šæ”¾ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡ä¸€ç›´å ç”¨ï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨å®Œä¹‹åï¼Œè¦é€šè¿‡cudaDeviceResetæ¥é‡Šæ”¾ä¸Šä¸‹æ–‡
           cudaSetDevice(device);
           cudaError_t cuda_status = cudaMemGetInfo(&avail,&total);
           if (cudaSuccess != cuda_status)
           {
               std::cout << "Error: cudaMemGetInfo fails : " << cudaGetErrorString(cuda_status) << std::endl;
               return false;
           }
           double freeMemory = double(avail) / (1024.0 * 1024.0);     // MB
           if(freeMemory > maxRestMemory){
               maxRestMemoryDevice = device;
               maxRestMemory = freeMemory;
           }
           //cudaDeviceResetæ¥é‡Šæ”¾setDeviceé€ æˆçš„ä¸Šä¸‹æ–‡
           cudaDeviceReset(device);
       }
       if(maxRestMemoryDevice != -1){
           cudaSetDevice(maxRestMemoryDevice);
           return true;
       }
       return false;
   }
   ```

6. tensorRTåœ¨è¿è¡Œæ—¶æŠ¥`[W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See "Lazy Loading" section of CUDA documentation `

   - https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading

   - æ‡’åŠ è½½æ— æ³•å¯åŠ¨ï¼Œä½¿èƒ½å®ƒå¯ä»¥æ˜¾è‘—çš„é™ä½è®¾å¤‡çš„å†…å­˜å ç”¨åŠ é€ŸtensortRTçš„åˆå§‹åŒ–ã€‚

   - å› ä¸ºæˆ‘è¿è¡Œçš„æ—¶å€™ï¼ŒæŠ¥è¿™ä¸ªè­¦å‘Šï¼Œæˆ‘ä¼šå‘ç°æˆ‘çš„ç¨‹åºæ‰€å ç”¨çš„è®¾å¤‡å†…å­˜ï¼ˆ480MBï¼‰æ¯”æ²¡æœ‰æŠ¥è¿™ä¸ªè­¦å‘Šçš„æ—¶å€™æ‰€å ç”¨çš„è®¾å¤‡å†…å­˜ï¼ˆ194MBï¼‰è¦å·®ä¸å¤šå¤§ä¸€åŠï¼Œæ‰€ä»¥æˆ‘å¿…é¡»è§£å†³è¿™ä¸ªè­¦å‘Šã€‚

   - ![](./legend/warningandnowarning.png)

   - é¦–å…ˆcuda toolkitå¿…é¡»å¤§äº11.7ï¼Œå…¶æ¬¡éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡CUDA_MODULE_LOADING = LAZYï¼Œç„¶åå†æ¬¡è¿è¡Œå°±ä¸ä¼šæŠ¥è­¦å‘Šï¼Œè®¾å¤‡å†…å­˜çš„å ç”¨ä¹Ÿæ¢å¤æ­£å¸¸ã€‚

   - ```bash
     vim ~/.bashrc
     export CUDA_LAZY_LOADING="LAZY"
     
     source ~/.bashrc
     
     # çœ‹åˆ°ç»“æœï¼Œå‘ç°è®¾ç½®æˆåŠŸ
     env | grep CUDA_LAZY_LOADING
     CUDA_MODULE_LOADING=LAZY
     
     # å†æ¬¡è¿è¡ŒTensorRTç¨‹åºå°±ä¸ä¼šæŠ¥è­¦å‘Šäº†
     ```

   - å‚è€ƒï¼š[CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed](https://blog.csdn.net/s1_0_2_4/article/details/135026761)

7. dockerè¿è¡Œgpué•œåƒåï¼ˆ--gpus allï¼‰ï¼Œä¸€æ®µæ—¶é—´åæ— æ³•åœ¨å®¹å™¨å†…æ‰§è¡Œ`nvidia-smi`ï¼ŒæŠ¥`Docker with GPU: "Failed to initialize NVML: Unknown Error"`

   - [å‚è€ƒ1](https://www.cnblogs.com/azureology/p/16673192.html)

   ```
   ä¼˜å…ˆå°è¯•å°†hostæ˜¾å¡é©±åŠ¨ç‰ˆæœ¬ä¸å†…æ ¸å¯¹é½ï¼Œæ–¹æ³•ä¸ºsudo dmesg | grep NVRMæŸ¥çœ‹æ­£ç¡®ç‰ˆæœ¬ã€‚å¦‚æœä¸è¡Œå†å¾€ä¸‹çœ‹ï¼š
   
   è¿™ä¸ªé—®é¢˜æ¯”è¾ƒå¤æ‚ï¼Œåœ¨æˆ‘è¿™è¾¹å±äºå¶å‘ï¼Œè¡¨ç°ä¸ºcontainerä¸­cudaæ— æ³•æ­£å¸¸è°ƒç”¨ï¼Œè¿è¡Œnvidia-smiæŠ¥é”™Docker with GPU: "Failed to initialize NVML: Unknown Error"ï¼Œä¸”è°ƒç”¨torch.cuda.is_avaliable()å€¼ä¸ºFalseï¼Œé‡å¯containerå¯ä»¥æš‚æ—¶æ¢å¤æ­£å¸¸ï¼Œhostå§‹ç»ˆæ²¡æœ‰å‘ç”Ÿæ­¤ç±»é—®é¢˜ã€‚
   
   å¼€å§‹æ ¹æ®å‚è€ƒæ–‡æ¡£æŒ‡å¼•ä¿®æ”¹äº†cgroupså‚æ•°ï¼Œæ­£å¸¸ä½¿ç”¨ä¸€æ®µæ—¶é—´åé—®é¢˜ä¾æ—§ï¼Œåç»­åˆå°†hostå®‰è£…çš„nvidiaé©±åŠ¨å‡çº§åˆ°æœ€æ–°ä¾ç„¶æ— æ³•è§£å†³ã€‚
   
   ä»”ç»†è§‚å¯Ÿæ–‡æ¡£ä¸­çš„docker runå‘½ä»¤ï¼ŒåŒ…å«--gpus allå¤–è¿˜æœ‰--privilegedä¸çŸ¥ç”¨æ„ä¸ºä½•ã€‚
   æŠ±ç€è¯•è¯•çœ‹çš„å¿ƒæ€å°†ç°æœ‰containeræ‰“åŒ…ä¸ºimageå¹¶é‡æ–°åŠ å…¥--privilegedå‚æ•°ï¼Œé—®é¢˜æœªå†å¤ç°
   ```

   - [å‚è€ƒ2](https://blog.csdn.net/ZnS_oscar/article/details/134108758)

8. 



